---
title: "S02Var02"
author: "Daniel Craig"
date: "2023-06-15"
output: html_document
---
#To do list:
1. Compare auto.arima with log version of the data
2. Compare auto.arima with means instead of removed NAs
3. Try a (2,0,0) model
4. Try a seasonal model?
5. tsCV - Cross Validate
6. test/train


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(forecast) #accuracy
library(tidyverse)
library(readr)
library(e1071) #skewness
library(urca) #for ur.kpss()

#data <- read_csv("C:\\Users\\dcrai\\source\\repos\\DATA624\\Project 1\\Data Set for Class.csv", col_types = cols(SeriesInd = col_date(format = "%m/%d/%y")))

data <- read_csv("C:\\Users\\dcrai\\source\\repos\\DATA624\\Project 1\\Data Set for Class.csv")

dataS01 <- data %>% filter(category == 'S01') %>% select(SeriesInd, category, Var02)
```
No NA's were present amongst the dataset, only the predictions (1623 - 1762) were NA.
```{r Removing NAs}
#NA
which(is.na(dataS01$Var02))

#Removing the 140 empty values
dataS01 <- dataS01[-c(1623:1762),]

```

Below is code chunk for checking skewness. Both with skewness() and the ratio between maximum and minimum of the variable. Benchmark is if the ratio is equal to 20 or high. With such low skewness, no transformation was chosen to retain interpretability of the data.
```{r Skewness}
#Skewness
skewness(dataS01$Var02)
max(dataS01$Var02)/min(dataS01$Var02) #Checked using the benchmark of ratio of max to min >= 20
#BoxCoxTrans(dataS01$Var02) #Checking what BoxCox would do, its a log
```
There were two significant outliers, due to those it seemed a log transformation was enough to deal with them.
```{r Full Data TimeSeries}
tsS01v2 <- ts(dataS01$Var02,
              start = min(dataS01$SeriesInd),
              frequency =1)

#length(tsS01v2) check for 1622

ltsS01v2 <- log(tsS01v2)


autoplot(ltsS01v2) + ylab("Var01") +xlab("SeriesIndex")
```


I tested for seasonality both by reformatting the SeriesInd index as date, as well as testing weekly data since the indexes skipped weekends. Also tested other numbers like 7, 4, 12, 24, etc.
```{r Seasonality Checking}
# season5TS <- ts(dataS01$Var01,
#               start = min(dataS01$SeriesInd),
#               frequency =5) #Weekly
# 
# season20TS <- ts(dataS01$Var01,
#               start = min(dataS01$SeriesInd),
#               frequency =20) #Monthly
# 
# season60TS <- ts(dataS01$Var01,
#               start = min(dataS01$SeriesInd),
#               frequency =60) #Quarterly
# 
# season240TS <- ts(dataS01$Var01,
#               start = min(dataS01$SeriesInd),
#               frequency =240) #Yearly
# 
# nsdiffs(season5TS)
# nsdiffs(season20TS)
# nsdiffs(season60TS)
# nsdiffs(season240TS)

```


```{r Train/Test Split}

# 1622 observations, 70/30 split, 1135 is the 70th percentile obs
valsToPredict70 <- 1622 - round(1622*.7)
valsToPredict80 <- 1622 - round(1622*.8)

#40669 was start id in the time series
endValTS70 <- 40669 + round(1622*.7)
endValTS80 <- 40669 + round(1622*.8)

#Checking indexes again
# length(dataS01$SeriesInd)
# dataS01[1135,1]
# dataS01[1622,1]


#Create splits
train70 <- window(ltsS01v2, end = endValTS70)
train80 <- window(ltsS01v2, end = endValTS80)
```
On the training data, something akin to a (1,1,3) model is what I'm expecting. ACF has 1 significant spike at lag 1, PACF has the first 4. Could also be a (0,0,3)
```{r train ACF Plots}
train70 %>% diff() %>%
  ggtsdisplay(xlab="Index",
    main="Var01")

train70 %>% ur.kpss() %>% summary()
ndiffs(train)


#80 Split
train80 %>% diff() %>%
  ggtsdisplay(xlab="Index",
    main="Var01")

train80 %>% ur.kpss() %>% summary()
ndiffs(train)


```
```{r nonSeasonal ARIMA All}

testNonSeasonalArima <- function(data) {
  #Instantiate a list
  models <- list()

  # Iterate through each of the potential values for p,d,q
  for (p in 0:3) {
    for (d in 0:1) {
      for (q in 0:3) {
        # Skip the (0,0,0) model as it is equivalent to the mean
        if (p == 0 && d == 0 && q == 0) {
          next
        }
        
        # Fit the ARIMA models
        model <- tryCatch({
          arimaModel <- Arima(data, order = c(p, d, q), include.mean = TRUE)
          modelName <- paste0("ARIMA(", p, ",", d, ",", q, ")")
          models[[modelName]] <- arimaModel
        }, error = function(e) {
          NULL
        })
      }
    }
  }

  # Return the list of models
  return(models)
}


testNonSeasonalArima(train70)
# train70
# ARIMA112 542.31 
# ARIMA113 541.69
# ARIMA212 539.93
# ARIMA313 541.11

testNonSeasonalArima(train80)
# ARIMA312   652.06
# ARIMA212   650.31
# ARIMA113   651.59

```

Auto ARIMA recommends ARIMA212 for both training splits. Some others to try are 313, 113, 212. No models pass the Ljung-Box test and most have two spikes in the ACF
```{r Train AutoArima}
(trainAuto70 <- auto.arima(train70, stepwise=FALSE, approximation = FALSE))
#539.98  ARIMA2,1,2

(trainAuto80 <- auto.arima(train80, stepwise=FALSE, approximation = FALSE))
#650.31 ARIMA212

trainAuto70 %>% forecast() %>% autoplot()

trainAuto80 %>% forecast() %>% autoplot()

checkresiduals(trainAuto70, lag = 50)
checkresiduals(trainAuto80, lag =50)



(train70Arima113 <- Arima(train70, order = c(1,1,3)))
checkresiduals(train70Arima113, lag = 50)
(train70Arima212 <- Arima(train70, order = c(2,1,2)))
checkresiduals(train70Arima212, lag = 50)
(train70Arima313 <- Arima(train70, order = c(3,1,3)))
checkresiduals(train70Arima313, lag = 50)

(train80Arima113 <- Arima(train80, order = c(1,1,3)))
checkresiduals(train80Arima113, lag = 50)
(train80Arima212 <- Arima(train80, order = c(2,1,2)))
checkresiduals(train80Arima212, lag = 50)
(train80Arima313 <- Arima(train80, order = c(3,1,3)))
checkresiduals(train80Arima313, lag = 50)
```
Ultimately, the ARIMA211
```{r Accuracy Check}

(accTrain70 <- trainAuto70 %>% forecast(h = valsToPredict70))  #ARIMA211
 #MAPE of 2.312 on testing, we want something close to 0. Can go up to 100

predAuto70 <- trainAuto70 %>% forecast(h = valsToPredict70)
predAuto70 <- exp(predAuto70$mean)
(mape <- accuracy(predAuto70, tsS01v2)[, "MAPE"])     #30.05  ARIMA(2,1,1)

(accTrain80 <- trainAuto80 %>% forecast(h = valsToPredict80) %>%
  forecast::accuracy(ltsS01v2))    #MAPE of 2.148718, which is a bit better.

predAuto80 <- trainAuto80 %>% forecast(h = valsToPredict80)     #ARIMA213
predAuto80 <- exp(predAuto80$mean)
(mape <- accuracy(predAuto80, tsS01v2)[, "MAPE"])      #32.24  ARIMA(2,1,3)



#Out of the below models, the Arima212 performed best at 2.314 on the test set
accTrain70Arima113 <- train70Arima113 %>% forecast(h = valsToPredict70) 

pred11370 <- train70Arima113 %>% forecast(h = valsToPredict70)
pred11370 <- exp(pred11370$mean)
(mape <- accuracy(pred11370, tsS01v2)[, "MAPE"])      #30.64



(accTrain70Arima212 <- train70Arima212 %>% forecast(h = valsToPredict70))

pred21270 <- train70Arima212 %>% forecast(h = valsToPredict70)
pred21270 <- exp(pred21270$mean)
(mape <- accuracy(pred21270, tsS01v2)[, "MAPE"])      #30.506



(accTrain70Arima313 <- train70Arima313 %>% forecast(h = valsToPredict70))

pred31370 <- train70Arima313 %>% forecast(h = valsToPredict70)
pred31370 <- exp(pred31370$mean)
(mape <- accuracy(pred31370, tsS01v2)[, "MAPE"])      #32.779


#Best here was ARIMA313 at 2.148
(accTrain80Arima113 <- train80Arima113 %>% forecast(h = valsToPredict80))


pred113 <- train80Arima113 %>% forecast(h = valsToPredict80)
pred113 <- exp(pred113$mean)
(mape <- accuracy(pred113, tsS01v2)[, "MAPE"])  #32.27043



(accTrain80Arima212 <- train80Arima212 %>% forecast(h = valsToPredict80))

pred212 <- train80Arima212 %>% forecast(h = valsToPredict80)
pred212 <- exp(pred212$mean)
(mape <- accuracy(pred212, tsS01v2)[, "MAPE"])  #32.247

(accTrain80Arima313 <- train80Arima313 %>% forecast(h = valsToPredict80)) 
pred313 <- train80Arima313 %>% forecast(h = valsToPredict80)
pred313 <- exp(pred313$mean)
(mape <- accuracy(pred313, tsS01v2)[, "MAPE"]) #32.259


indexInExcel70 <- dataS01[round(1622*.7),1]
val70Pred <- 1622 - round(1622*.7)

indexInExcel80 <- dataS01[round(1622*.8),1]
val80Pred <- 1622 - round(1622*.8)
#indexCutoff : Calculating the SeriesIndex the training set is ending on so that we can tell what points our forecast correspond with when we make our predictions
# The Time Series doesn't recognize the 2 day skip, so it causes issues
# To compare our predictions to the excel sheet it starts at 42315
# the last SeriesIndex used was 42315
# the last SeriesIndex is 43021

#For printing predictions from the train/test partition
# autoTrainPoint70 <- trainAuto70 %>% forecast(h = 487)
# write_csv(data.frame(autoTrainPoint70$mean), "C:\\Users\\dcrai\\source\\repos\\DATA624\\Project 1\\S01Var0170 ARIMA211 Train Forecast.csv")
# 
# autoTrainPoint80 <- trainAuto80 %>% forecast(h = 324)
# write_csv(data.frame(autoTrainPoint80$mean), "C:\\Users\\dcrai\\source\\repos\\DATA624\\Project 1\\S01Var0180 ARIMA312 Train Forecast.csv")

```

The ETS(A,N,N) model on the 80 set out performed the ARIMA models with a MAPE of 2.1477, but its AIC is so high we will stick with the ARIMA313 model for predictions
```{r ETS}

etsModel70 <- ets(train70)
etsModel80 <- ets(train80)

ets70 <- etsModel70 %>% forecast(h = val70Pred)

etsPred70 <- etsModel70 %>% forecast(h = val70Pred)
etsPred70 <- exp(etsPred70$mean)
(mape <- accuracy(etsPred70, tsS01v2)[, "MAPE"])     #34.81



ets80 <- etsModel80 %>% forecast(h = val80Pred)

etsPred80 <- etsModel80 %>% forecast(h = val80Pred)
etsPred80 <- exp(etsPred80$mean)
(mape <- accuracy(etsPred80, tsS01v2)[, "MAPE"])       #32.20935

```

# Naive & Average
Naive performed well at 1.72 for the 80 data split, but 
```{r}
# Fit the Naive model
naiveModel70 <- naive(train70, h = val70Pred)
naiveModel80 <- naive(train80, h = val80Pred)



# Compare Predictions
naiveModel70preds <- naiveModel70 %>% forecast(h = 140)
naiveModel70Exp <- exp(naiveModel70preds$mean)
(mape <- accuracy(naiveModel70Exp, tsS01v2)[, "MAPE"])      #31.96372

naiveModel80preds <- naiveModel80 %>% forecast(h = 140 )
naiveModel80Exp <- exp(naiveModel80preds$mean)
(mape <- accuracy(naiveModel80Exp, tsS01v2)[, "MAPE"])      #36.1954

#Average version
mean70 <- forecast(mean(train70), h = 1622)
forecast(mean(train80), h = 1622) %>% forecast::accuracy(tsS01v2)

mean70Exp <- exp(mean70$mean)
(mape <- accuracy(mean70Exp, tsS01v2)[, "MAPE"]) #52.969


mean80Exp <- exp(mean80$mean)

length(mean70Exp)
length(mean80Exp)


test70 <- tsS01v2[1136:1622]
test80 <- tsS01v2[1299:1622]

meanMape70 <- mean(abs(mean70Exp - test70) / test70, na.rm = TRUE) #59546083?
meanMape80 <- mean(abs(mean80Exp - test80) / test80, na.rm = TRUE)


# forecast(naiveModel70, h = 140) %>% forecast::accuracy(ltsS01v2)
# 
# forecast(mean(train70), h = 1622) %>% forecast::accuracy(ltsS01v2)
# forecast(mean(train80), h = 1622) %>% forecast::accuracy(ltsS01v2)

```
## Full Model
This full model shows a non-stationary series with clear long-running trends. A difference is needed and is confirmed by the KPSS Unit Root Test & `ndiffs`. ACF and PACF both show 2 spikes at lags 1 and 2. Suggesting a (0,1,2) or (2,1,0). We will use our trained model of (2,1,1)
```{r Full Model ACF Plots}
ltsS01v2 %>% diff() %>%
  ggtsdisplay(xlab="Index",
    main="Var01")

ltsS01v2 %>% ur.kpss() %>% summary()
ndiffs(ltsS01v2)
#nsdiffs(ltsS01v2)
```

```{r 211 ARIMA Final Preds}
# arima313 <- Arima(ltsS01v2, order = c(3,1,3))
# 
# arima313Preds <- arima313 %>% forecast(h = 140)
# 
# #Note we exponentiated our predictions 
# write_csv(data.frame(exp(arima313Preds$mean)), "C:\\Users\\dcrai\\source\\repos\\DATA624\\Project 1\\S01Var02Final ARIMA313 Forecast.csv")

final211 <- Arima(tsS01v2, order = c(2,1,1))

final211Preds <- final211 %>% forecast(h = 140)

final211Preds$mean

write_csv(data.frame(final211Preds$mean), "C:\\Users\\dcrai\\source\\repos\\DATA624\\Project 1\\S01Var02Final ARIMA211 Forecast.csv")

```