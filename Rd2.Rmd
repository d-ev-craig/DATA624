---
title: "Project Notes"
author: "Daniel Craig"
date: "2023-07-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(e1071)#skewness()
library(earth)#?
library(mice) #for imputing
library(MLmetrics) #for MAPE
library(pls) # for pls
library(lars) # for lars
library(elasticnet) # for ridge

```

Todo List:
1. Process data

2. Partition Data

3. Variable Importance
- Handle highly correlated variables

4. Model stuff out

Stuff I'd like to see on a graph in EDA:
1. Skewness of some variables
2. Normality of some variables
3. Correlation of variables


Attempting different methods list:
1. Removing Hyd Pressure 1 - we left this in, its near zero var
2. Leaving the dummy categorical variables in fraction form, imputing them put them to fractions, we chose to round them to either 0 or 1
3. Remove the "kendall" method in correlation and deal with the highly correlated variables


PLS
Penalty ( LARS, Lasso, Ridge)
RandomForest


## Processing
```{r}
rawData <- read_csv("C:\\Users\\dcrai\\source\\repos\\DATA624\\Project 2\\Materials\\StudentData - TO MODEL.csv")

rawData$`Brand Code` <- as_factor(rawData$`Brand Code`)

colnames(rawData)[1:33] <- c("brandCode","carbVol","fillOz","pcVol","carbPressure","carbTemp","PSC","pscFill","pscCO2","mnfFlow","carbPressure1","fillPressure","hydPressure1","hydPressure2","hydPressure3","hydPressure4","fillerLvl","fillerSpeed","temp","usageCont","carbFlow","density","MFR","balling","pressureVacuum","PH","oxyFiller","bowlSetpoint","pressureSetpoint","airPressurer","alchRel","carbRel","ballingLvl")
```

```{r nzv multicol}
#Check for zero variance and multicol
nearZeroVar(rawData, names = TRUE) #Finds that column 13 is near zero variance, this is Hyd Pressure1

#In this version of the code, we will be removing hydPressure
cor(x = rawData$hydPressure1,y=rawData$PH, use = "complete.obs")


#No multi colinearity was found(integer(0) was returned)
findCorrelation(cor(x = rawData[,-1], use = "complete.obs"))


```

```{r NA Counts}
#Check for NAs
which(is.na(rawData[,1]))
which(is.na(rawData))

rawData[,26]

#Not that many are missing
missing <- unlist(lapply(rawData, function(x) sum(is.na(x))))/nrow(rawData)

md.pattern(rawData)
```

```{r miceImpute}
#Mice will handle transforming our brandCode column from a factor into dummy variables!
#It wont like it if we do that for it beforehand and pass it a data frame with 
#Imputation was handled using logreg for categorical variables and pmm from mice
# can make use of a procedure called predictive mean matching (PMM) to select which values are imputed. PMM involves selecting a datapoint from the original, nonmissing data which has a predicted value close to the predicted value of the missing sample

miceImpute <- mice(rawData)

#miceImpute2 <- mice(dummyData,method = miceImpute$method)

imputedData <- complete(miceImpute)
```
```{r Old CARET dummy Var}
#dummy vars even needed?
# dummies <- dummyVars(PH ~ ., data = rawData)
# 
# dummyData <- data.frame(predict(dummies, newdata = rawData))
# 
# 
# #Attempting to set the dummy variables as factors 
# dummyData[,1:4] <- apply(dummyData[,1:4], FUN = as)
# dummyData[,1:4] <- as.factor(dummyData[,1:4])
# 
# colnames(dummyData)[1:35] <- c("brandCodeB","brandCodeA","brandCodeC","brandCodeD","carbVol","fillOz","pcVol","carbPressure","carbTemp","PSC","pscFill","pscCO2","mnfFlow","carbPressure1","fillPressure","hydPressure1","hydPressure2","hydPressure3","hydPressure4","fillerLvl","fillerSpeed","temp","usageCont","carbFlow","density","MFR","balling","pressureVacuum","oxyFiller","bowlSetpoint","pressureSetpoint","airPressurer","alchRel","carbRel","ballingLvl")
# 
# 
# #Impute missing data
# imputeData <- preProcess(dummyData, method = c("knnImpute"))


#Our categorical variables are turned into fractions, thus we will transform them back into 0s and 1s

#Split into train and test
# imputedData <- imputeData$data
# 
# imputedData[,1:4] <- lapply(imputedData[,1:4], FUN = function(x) ifelse(x >= .5,1,0))
# 
# imputedData

```


```{r Processing Data}

#Below shows high skew in oxygen filler, temperature, air pressure, pscC02, etc.
skew <- apply(imputedData[,-1], FUN = function(x)skewness(x), MARGIN =2)
sort(skew, decreasing = TRUE)

#PH does not show significant skew, so we won't transform it

predictors <- imputedData %>% select(-PH)
response <- imputedData %>% select(PH)

processData <- preProcess(predictors, method = c("YeoJohnson","center","scale","pca","zv","nzv"))
#processData 17 variables had a Yeo-Johnson transformation


#I did not dummy variable the brandcode categorical variable before using PCA, thus it was ignored, how does this impact things?
processedData <- predict(processData, predictors)
processedData <- data.frame(response,processedData)

#YeoJohnson is BoxCox trans equivalent that allows negative values
#zv and nzv do not include zero variance and near zero variance variables but weren't put into the method
#using PCA since a lot of highly correlated variables are around

```


```{r Partitioning Data}
trainPart <- createDataPartition(processedData[,1], p=0.7, list=F)

train <- processedData[trainPart, ]
test <- processedData[-trainPart, ]

```
## Model Fitting

I want to check a few that will show us importance of variables. Let's start with random forest since we can see variable importance.

### PLS/Penalized

```{r}
plsProcess <- preProcess(imputedData, method = c("YeoJohnson","center","scale","zv","nzv"))
plsData <- predict(plsProcess,imputedData)


trainPLSPart <- createDataPartition(plsData[,26], p=0.7, list=F)

trainPLS <- plsData[trainPLSPart, ]
testPLS <- plsData[-trainPLSPart, ]


set.seed(100)
ctrl <- trainControl(method = "cv", number =10)

plsTune <- train(trainPLS[,-26], trainPLS[,26], 
                 method = "pls", 
                 tuneLength = 33,  
                trControl = ctrl
)

plsPreds <- predict(plsTune, testPLS)

#plsTune #ncomp = 13
min(plsTune$results$RMSE)
max(plsTune$results$Rsquared)

postResample(pred = plsPreds, obs = testPLS$PH)

```
### LARS
```{r}
#plsData has been centered, scaled, YeoJohnson'd, zv'd, and nzv'd
larsData <- dummyVars(PH ~ .,plsData)
dummyLarsData <- as.data.frame(predict(larsData,plsData))
dummyLarsData <- cbind(PH=plsData$PH,dummyLarsData)

trainLarsPart <- createDataPartition(dummyLarsData[,1], p=0.7, list=F)

trainLars <- dummyLarsData[trainLarsPart, ]
testLars <- dummyLarsData[-trainLarsPart, ]

larsTune <- train(trainLars[,-1], trainLars[,1], 
                 method = "ridge", 
                 tuneLength = 33,  
                trControl = ctrl
)

larsPreds <- predict(larsTune, testLars)

#larsTune #lambda = .0068
min(larsTune$results$RMSE)
max(larsTune$results$Rsquared)

postResample(pred = larsPreds, obs = testLars$PH)


```

#### Ridge

```{r}

ridgeGrid <- data.frame(.lambda = seq(0, .1, length = 15))  
set.seed(100) 
ridgeRegFit <- train(trainLars[,-1],trainLars[,1],
                     method = "ridge",
                     tuneGrid = ridgeGrid,
                     trControl = ctrl
)

#ridgeRegFit

min(ridgeRegFit$results$RMSE)
max(ridgeRegFit$results$Rsquared)

ridgePreds <- predict(ridgeRegFit, testLars)

postResample(pred = ridgePreds, obs = testLars$PH)

```

#### Lasso

```{r}

lassoGrid <- expand.grid(alpha = 1,
                         lambda = seq(0.01, 1, by = 0.01))

set.seed(100) 
lassoFit <- train(trainLars[,-1],trainLars[,1],
                     method = "glmnet",
                     tuneGrid = lassoGrid,
                     trControl = ctrl
)

#lassoFit

min(lassoFit$results$RMSE)
max(lassoFit$results$Rsquared)

lassoPreds <- predict(lassoFit, testLars)

postResample(pred = lassoPreds, obs = testLars$PH)
```

### Forests
```{r}
#rpartTree <- rpart(PH ~., data = train)
ctrl <- trainControl(method = "cv", number =10)
tuneDepth <- expand.grid(maxdepth= seq(1,10,by=1))
tuneCP <- expand.grid(cp = c(.01,.05,.1,.2))


#method = "rpart"
rpartTree <- train(PH ~., 
                 data = train, 
                 method = "rpart",
                 tuneGrid = tuneCP,
                 trControl = ctrl)

(cartCPRMSE <- min(rpartTree$results$RMSE))
(cartCPRsq  <- max(rpartTree$results$Rsquared))

#rpart2 to train over max depth
rpart2Tree <- train(PH ~., 
                 data = train, 
                 method = "rpart2",
                 tuneGrid = tuneDepth,
                 trControl = ctrl) 

(cartDepthRMSE <- min(rpart2Tree$results$RMSE))
(cartDepthRsq  <- max(rpart2Tree$results$Rsquared))

#Complexity Parameter performed better(rpartTree), we will use that one in comparisons, both performed poorly
```

```{r M5 Reg Model}
library(RWeka)
yesno <- c("Yes", "No")
tuneM5 <- expand.grid(pruned = yesno, smoothed = yesno, rules = yesno)

m5Tune <- train(PH ~.,
                data = train,
                 method = "M5", #or "M5Rules"
                 trControl = ctrl,
                control = Weka_control(M=10),
                tuneGrid = tuneM5
                 )

m5RMSE <- min(m5Tune$results$RMSE)
m5Rsq <- max(m5Tune$results$Rsquared)


tuneM5Rules <- expand.grid(pruned = yesno, smoothed = yesno)
m5RulesTune <- train(PH ~.,
                data = train,
                 method = "M5Rules", #or "M5Rules"
                 trControl = ctrl,
                control = Weka_control(M=10),
                tuneGrid = tuneM5Rules
                 )

#M5 metrics
min(m5Tune$results$RMSE)
max(m5Tune$results$Rsquared)

#M5 Rules outperformed on RMSE, but lost a substantial amount in Rsqr, thus we will not use the Rules version
min(m5RulesTune$results$RMSE) #better by .02
min(m5RulesTune$results$Rsquared) #worse by .2
```


```{r BAG}
library(kernlab)
n <- ncol(train)
tuneVars <- data.frame(vars = seq(2,n,10))

predfunct<-function (object, x)
{
 if (is.character(lev(object))) {
    out <- predict(object, as.matrix(x), type = "probabilities")
    colnames(out) <- lev(object)
    rownames(out) <- NULL
  }
  else out <- predict(object, as.matrix(x))[, 1]
  out
}


svmBagCtrl <- bagControl(fit = svmBag$fit,
                      predict = predfunct,
                      aggregate= svmBag$aggregate)

citBagCtrl <- bagControl(fit = ctreeBag$fit,
                      predict = ctreeBag$pred,
                      aggregate= ctreeBag$aggregate)

#Attempted using SVM bag ctrl method but it gave many warnings/errors

svmBagTune <- train(PH ~.,
                  data = train,
                  method = "bag",
                  trControl = trainControl(method = "boot",number = 10),
                  tuneGrid = tuneVars,
                  bagControl= svmBagCtrl
                  )

citBagTune <- train(PH ~., 
                  data = train,
                  method = "bag",
                  trControl = trainControl(method = "boot",number = 10),
                  tuneGrid = tuneVars,
                  bagControl= citBagCtrl
                  )

#Looks like svm bag tune did better
(svmBagRMSE <- min(svmBagTune$results$RMSE))
(svmBagRsq <- min(svmBagTune$results$Rsquared))

(bagRMSE <- min(citBagTune$results$RMSE))
(bagRsq <- min(citBagTune$results$Rsquared))

```

```{r RandomForest}

gridRF <- expand.grid(mtry = seq(2,20, by = 2))

tuneRF <- train(PH ~., 
                  data = train,
                  method = "rf",
                  trControl = trainControl(method = "boot",number = 10),
                  tuneGrid = gridRF,
                  bagControl= citBagCtrl
                  )
(rfRMSE <- min(tuneRF$results$RMSE))
(rfRsq <- min(tuneRF$results$Rsquared))
```

```{r SGB}
#Example grid from book, but added n.minobsinnode
gbmGrid <- expand.grid(
  interaction.depth = seq(1,7, by = 2),
  n.trees = seq(100,1000,by = 50),
  shrinkage = c(.01,.1),
  n.minobsinnode=10
  )

sgbTune <- train(PH ~., data = train,
                 method = "gbm", 
                 tuneGrid = gbmGrid,
                 trControl = trainControl(method = "boot", number = 10),
                 verbose=F)

(sgbRMSE <- min(sgbTune$results$RMSE))
(sgbRsq <- max(sgbTune$results$Rsquared))
```

```{r Cubist}

cubistGrid <- expand.grid(
  committees = seq(1,100, by = 10),
  neighbors = seq(1,10, by =2)
  )

cubistTune <- train(PH ~., data = train,
                 method = "cubist", 
                 tuneGrid = cubistGrid,
                 trControl = trainControl(method = "boot", number = 10)
                )

(cubistRMSE <- min(cubistTune$results$RMSE))
(cubistRsq <- max(cubistTune$results$Rsquared))
```

```{r Train Acc Dataframe}
trainAcc <- data.frame( RMSE = c(cartDepthRMSE,m5RMSE,svmBagRMSE,bagRMSE,rfRMSE,sgbRMSE,cubistRMSE),
                        Rsq = c(cartDepthRsq,m5Rsq,svmBagRsq,bagRsq,rfRsq,sgbRsq,cubistRsq),
                        row.names = c("CART","M5 No Rules","svmBag","ci","RF","SGB","Cubist")
                        )

trainAcc
```

```{r Test Predictions}

# Transform, Center, Scale

#Get our processed data back, this doesn't create new predictions since we paired it with our test dataframe
testNoYield <- predict(procData,test[,-1])

#Removing the same variables with near zero variance and multi-coll
testNoYield <- testNoYield[ ,-c(nearZeroVars, multiCol)]


#PH is 26th column, we remove
cartPreds <- predict(rpartTree, newdata = test[,-26])
cart2Preds <- predict(rpart2Tree, newdata = test[,-26])

m5TunePreds <- predict(m5Tune, newdata = test[,-26])
m5RulesTunePreds <- predict(m5RulesTune, newdata = test[,-26])

citBagPreds <- predict(citBagTune, newdata = test[,-26])
svmBagPreds <- predict(svmBagTune, newdata = test[,-26])

rfPreds <- predict(tuneRF, newdata = test[,-26])
sgbPreds <- predict(sgbTune, newdata = test[,-26])
cubistPreds <- predict(cubistTune, newdata = test[,-26])

```

```{r Test Accuracy}
accTest <- data.frame(rbind(
  postResample(pred = cartPreds, obs = test$PH),
  postResample(pred = cart2Preds, obs = test$PH),
  postResample(pred = m5TunePreds, obs = test$PH),
  postResample(pred = m5RulesTunePreds, obs = test$PH),
  postResample(pred = citBagPreds, obs = test$PH),
  postResample(pred = svmBagPreds, obs = test$PH),
  postResample(pred = rfPreds, obs = test$PH),
  postResample(pred = sgbPreds, obs = test$PH),
  postResample(pred = cubistPreds, obs = test$PH)),
  row.names =  c("CART CP","CART DEPTH","M5","M5NoR","CitBAG","SVM Bag","RF","SGB","CUBIST")
  )

preds <- data.frame(cart=cartPreds,cart2=cart2Preds,m5=m5TunePreds,m5R=m5RulesTunePreds,citBag=citBagPreds,svmBag=svmBagPreds,rf=rfPreds,sgb=sgbPreds,cubist=cubistPreds)

preds2 <- data.frame(score = ,
                  row.names = c("CART CP","CART DEPTH","M5",
                                "M5R","CitBAG","SVM Bag",
                                "RF","SGB","CUBIST")
)

mapePreds <- function(x) {
  MAPE(x,test$PH)
}

(mapeResults <- apply(preds, FUN = mapePreds, MARGIN = 2))

mapeResults <- data.frame(MAPE = c(mapeResults[1],mapeResults[2],mapeResults[3],mapeResults[4],mapeResults[5],
                     mapeResults[6], mapeResults[7], mapeResults[8],mapeResults[9]),
                  row.names = c("CART CP","CART DEPTH","M5",
                                "M5R","CitBAG","SVM Bag",
                                "RF","SGB","CUBIST")
)

accTestFull <- data.frame(accTest, MAPE = mapeResults)

accTestFull %>% arrange(MAPE, RMSE, MAE, desc(Rsquared))
#Remember we want:
# RMSE - small
# MAPE - small
# MAE - small
# Rsq - Big

#Look like RF performed the best with Cubist and M5 runner ups
```

## Non Tree Models

### Neural Net

```{r}
ctrl <- trainControl(method = "cv", number = 10)

##The findCorrelation takes a correlation matrix and determines the 
##column numbers that should be removed to keep all pair-wise correlations below a threshold
(tooHigh <- findCorrelation(cor(train[,-1]), cutoff = .75))#none were found too high
# >  [1] 14 13 23 30 21 32 31 16 17  5

findCorrelation(cor(x = rawData[,-1], use = "complete.obs"), cutoff = .75) # these are fewer
# >  [1]  9 23 14 30 32 31 21 11 16 17  5

(tooHigh <- findCorrelation(cor(rawData[,-1]), cutoff = .75))




##Create a specific candidate set of models to evaluate:
nnetGrid <- expand.grid(  .decay = c(0, 0.01, .1), 
                          .size = c(1:10),  
                          .bag = FALSE) #option is to use bagging instead of different random seeds.  
set.seed(100) 
nnetTune <- train(train[,-26], train[,26],
                  method = "avNNet",  
                  tuneGrid = nnetGrid,  
                  trControl = ctrl,
                  linout = TRUE, trace = FALSE, 
                  MaxNWts = 10 * (ncol(train[,-26]) + 1) + 10 + 1, maxit = 500)



nnPreds <- predict(nnetTune, newData = test$PH)

postResample(pred = nnPreds, obs = test$PH)
```

### MARS

```{r}
 #Define the candidate models to test with options from earth package
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)
set.seed(100)   #Fix the seed so that the results can be reproduced
marsTuned <- train(train[,-26], train[,26],  
                    method = "earth",  
                    tuneGrid = marsGrid,  #Explicitly declare the candidate models to test  
                    trControl = trainControl(method = "cv")
                   ) 

#Test set performance
marsPreds <- predict(marsTuned, newdata = test[,-26])

postResample(pred = marsPreds, obs = test$PH)

marsTuned$finalModel

varImp(marsTuned)
```


### SVM

```{r}
svmRTuned <- train(train[,-26], train[,26],
                   method = "svmRadial",
                   preProc = c("center", "scale"),
                   tuneLength = 14,
                   trControl = trainControl(method = "cv")
                   ) 
svmRTuned$bestTune

print(paste0("RMSE: ",round(min(svmRTuned$results$RMSE),2)))
print(paste0("Rsq: ",round(min(svmRTuned$results$Rsquared),2)))
print(paste0("MAE: ",round(min(svmRTuned$results$MAE),2)))
svmRTuned$finalModel



#Test set performance
svmRPreds <- predict(svmRTuned, newdata = test$PH)

postResample(pred = svmRPreds, obs = testData$PH)
```

### KNN

```{r}
library(caret)
knnModel <- train( train[,-26], train[,26],
                   method = "knn",
                   preProc = c("center","scale"),
                   tuneLength =10)



knnPred <- predict(knnModel, newdata = test$PH)

#postResample gets test set performance values
postResample(pred = knnPred, obs = test$PH)
```


#### Variable Importance
